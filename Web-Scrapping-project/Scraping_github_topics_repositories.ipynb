{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories for Topics on Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction: \n",
    "- GitHub is a platform for making and sharing computer programs. The project's main goal is to gather and organize information about GitHub topics and the programs (repositories) linked to them. It collects data about topic names, descriptions, and web addresses and also gets info about the programs like who made them, what they're called, how many people like them, and where to find them on the web. This data is structured and saved in a neat format for further study and analysis, helping us understand how topics are organized on GitHub and which programs are the most popular. All of this is done using a process called web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Scraping:\n",
    "- Web scraping, also known as web harvesting or web data extraction, is the process of automatically extracting data from websites. It involves parsing the HTML or other structured data on a web page and then collecting, transforming, and storing the desired information for various purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here are the steps we'll follow:\n",
    "\n",
    "1. We'll initiate the web scraping process by accessing the web page: \"https://github.com/topics.\"\n",
    "\n",
    "2. Next, we will retrieve a list of topics. For each topic, we aim to collect the topic title, the URL of the topic page, and a brief description of the topic.\n",
    "\n",
    "3. Within each topic, we'll delve further and retrieve information about the top 25 repositories within that topic, using the respective topic page.\n",
    "\n",
    "4. For each repository in the topic, we are interested in capturing the repository's name, the number of stars it has garnered, and its URL.\n",
    "\n",
    "5. Lastly, for each of the topics, we will organize the gathered information into a CSV (Comma-Separated Values) file. The CSV file will be structured in a tabular format, similar to this example:\n",
    "\n",
    "   ```\n",
    "   Name, Stars, URL\n",
    "   Repository 1, 100, https://github.com/repo1\n",
    "   Repository 2, 250, https://github.com/repo2\n",
    "   ...\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the list of Topics from Github\n",
    "1. Fetching the GitHub Topics Page:\n",
    "The script begins by fetching the GitHub \"Topics\" page by making an HTTP GET request using the `requests` library.\n",
    "2. Checking the Response and Parsing with `BeautifulSoup`:\n",
    "The script then checks the response to ensure it was successful (HTTP status code 200) and parses the HTML content of the page using `BeautifulSoup`.\n",
    "3. Extracting GitHub Topic Titles, Descriptions, and URLs:\n",
    "The script contains functions to extract the titles, descriptions, and URLs of GitHub topics.\n",
    "4. Organizing Data with `pandas`:\n",
    "The extracted data (titles, descriptions, and URLs) is then organized into a DataFrame using `pandas`.\n",
    "5. Scraping Repository Data for Each Topic:\n",
    "The script also includes functions to scrape data for repositories related to each topic. This involves making additional requests to specific topic pages, extracting repository information, and saving it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries:\n",
    "- os: This library allows you to interact with the operating system, particularly for tasks like file and directory operations.\n",
    "- pandas as pd: The pandas library is used for data manipulation and analysis, including working with structured data using DataFrames and Series.\n",
    "- requests: This library is used for making HTTP requests, enabling the script to fetch data from websites.\n",
    "- BeautifulSoup (from bs4): BeautifulSoup is a Python library for parsing and navigating HTML and XML documents. It's commonly used in web scraping to extract specific data from web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A URL for the GitHub \"Topics\" page is defined.\n",
    "The requests.get() function is used to make an HTTP GET request to the URL.\n",
    "The HTTP status code of the response is obtained using resp.status_code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://github.com/topics\"\n",
    "resp=requests.get(url)\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, We are extracting the text content from the HTTP response we received from the GitHub \"Topics\" page and then displaying the first 1000 characters of that content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_cont=resp.text\n",
    "page_cont[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using BeautifulSoup to create the `doc` object, we convert the raw HTML content in `page_cont` into a structured format, simplifying the process of exploring and extracting data from the web page. This parsed representation of the HTML content facilitates tasks such as web scraping and webpage analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc =  BeautifulSoup(page_cont, 'html.parser')\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a function to encapsulate the code for parsing HTML content using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_page():\n",
    "    topics_url='https://github.com/topics'\n",
    "    resp=requests.get(topics_url)\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(\"Failed to load page {}\".format(topics_url))\n",
    "    doc = BeautifulSoup(resp.text,'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topics_page()\n",
    "type(doc)\n",
    "doc.find('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we are using the BeautifulSoup object doc to find and extract the titles of topics from the parsed GitHub web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_titl_tags=doc.find_all('p',{'class':\"f3 lh-condensed mb-0 mt-1 Link--primary\"})\n",
    "print(len(topic_titl_tags))\n",
    "topic_titl_tags[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are extracting the text content (Topic title names) from the elements in topic_titl_tags and storing them in the topic_titles list. We can then use this list for further processing or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_titles=[]\n",
    "\n",
    "for i in topic_titl_tags:\n",
    "    topic_titles.append(i.text)\n",
    "\n",
    "print(topic_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are continuing to use the BeautifulSoup object 'doc' to find and extract specific HTML elements, specifically the descriptions of topics from the parsed web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_desc_tags=doc.find_all('p',{'class':\"f5 color-fg-muted mb-0 mt-1\"})\n",
    "print(len(topic_desc_tags))\n",
    "topic_desc_tags[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are collecting the text content (Description) of topic descriptions from a list of HTML elements and storing them in the topic_desc list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_desc=[]\n",
    "\n",
    "for i in topic_desc_tags:\n",
    "    topic_desc.append(i.text.strip())\n",
    "    \n",
    "print(topic_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we are extracting topic links from a GitHub web page. `topic_link_tags` will contain a list of matching `<a>` elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_link_tags=doc.find_all('a',{'class':\"no-underline flex-1 d-flex flex-column\"})\n",
    "len(topic_link_tags)\n",
    "topic_link_tags[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are storing the URLs of Topic titles from `topic_link_tags` into `topic_urls` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_urls=[]\n",
    "base_url= 'https://github.com'\n",
    "for i in topic_link_tags:\n",
    "    topic_urls.append(base_url+i['href'])\n",
    "\n",
    "print(topic_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, we can create functions to encapsulate the code for extracting topic titles, descriptions, and URLs from a web page.\n",
    "1. `get_topic_titles()` for topic titles\n",
    "2. `get_topic_desc()` for description of the topic\n",
    "3. `get_topic_urls()` for  links of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To get topic titles, we can pick `p` tags with the `class` ...\n",
    "\n",
    "![alt text](abcd.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    topic_titl_tags=doc.find_all('p',{'class':\"f3 lh-condensed mb-0 mt-1 Link--primary\"})\n",
    "    topic_titles=[]\n",
    "    for i in topic_titl_tags:\n",
    "        topic_titles.append(i.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_topic_titles(doc)\n",
    "print(len(titles))\n",
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. To get topic description, we can pick `p` tags with the `class` ...\n",
    "\n",
    "![alt text](Topic_desc.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_desc(doc):\n",
    "    topic_desc_tags=doc.find_all('p',{'class':\"f5 color-fg-muted mb-0 mt-1\"})\n",
    "    topic_desc=[]\n",
    "    for i in topic_desc_tags:\n",
    "        topic_desc.append(i.text.strip())\n",
    "    return topic_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = get_topic_desc(doc)\n",
    "print(len(desc))\n",
    "desc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To get topic urls, we can pick `a` tags with the `class` ... `no-underline flex-1 d-flex flex-column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    topic_link_tags=doc.find_all('a',{'class':\"no-underline flex-1 d-flex flex-column\"})\n",
    "    topic_urls=[]\n",
    "    base_url= 'https://github.com'\n",
    "    for i in topic_link_tags:\n",
    "        topic_urls.append(base_url+i['href'])\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = get_topic_urls(doc)\n",
    "print(len(urls))\n",
    "urls[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's consolidate all of this into a single function, which will structure the data and return it as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url='https://github.com/topics'\n",
    "    resp=requests.get(topics_url)\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(\"Failed to load page {}\".format(topics_url))\n",
    "    doc = BeautifulSoup(resp.text,'html.parser')\n",
    "    topics_dict = {\n",
    "        'titles' : get_topic_titles(doc),\n",
    "        'description' : get_topic_desc(doc),\n",
    "        'url' : get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the top repositorie from a topic page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `get_topic_page(topic_url)`: This function downloads and parses a web page related to a specified topic URL.\n",
    "\n",
    "- It sends an HTTP GET request to the topic URL.\n",
    "- Checks for a successful response (status code 200).\n",
    "- Parses the HTML content of the page using BeautifulSoup with the 'html.parser' parser.\n",
    "- Returns the parsed document as a BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    #Download the page\n",
    "    resp = requests.get(topic_url)\n",
    "    #Check successful response\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(\"Failed to load page {}\".format(topic_url))\n",
    "    #Parse using Beautiful soup\n",
    "    topic_doc=BeautifulSoup(resp.text,'html.parser')\n",
    "    return topic_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page('https://github.com/topics/3d')\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `parse_star_count(stars_str)`: This function parses star counts from strings.\n",
    "\n",
    "- Removes leading and trailing spaces from the input string.\n",
    "- Checks if the string ends with 'k' (indicating thousands).\n",
    "- Converts the string to an integer (e.g., \"1.5k\" to 1500) and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    stars_str=stars_str.strip()\n",
    "    if stars_str[-1]=='k':\n",
    "        return int(float(stars_str[:-1])*1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_tags=doc.find_all('span',{'class':\"Counter js-social-count\"})\n",
    "len(star_tags)\n",
    "parse_star_count(star_tags[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `get_repo_info(h3_tag, star_tag)`: This function returns information about a repository.\n",
    "\n",
    "- Extracts details such as username, repository name, repository URL, and star count from the provided HTML tags.\n",
    "- Returns these details as separate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url=\"https://github.com\"\n",
    "def get_repo_info(h3_tag,star_tag):\n",
    "    #return all the required info about a repository\n",
    "    a_tags=h3_tag.find_all('a')\n",
    "    username=a_tags[0].text.strip()\n",
    "    repo_name=a_tags[1].text.strip()\n",
    "    repo_url = base_url+a_tags[1]['href']\n",
    "    stars=parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name,stars,repo_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `get_topic_repos(topic_doc)`: This function extracts information about repositories related to a specific topic.\n",
    "\n",
    "- Finds and stores H3 tags containing repository titles, URLs, and usernames.\n",
    "- Finds star count tags for the repositories.\n",
    "- Creates a dictionary (topic_repos_dict) to store username, repository name, stars, and repository URL.\n",
    "- Iterates through the found tags to extract repository information and populates topic_repos_dict.\n",
    "- Returns the information as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    #Get the h3 tags containing repo title, repo URL and username\n",
    "    repo_tags=topic_doc.find_all('h3',{'class':\"f3 color-fg-muted text-normal lh-condensed\"})\n",
    "    #Get star tags\n",
    "    star_tags=topic_doc.find_all('span',{'class':\"Counter js-social-count\"})\n",
    "    #get repo info\n",
    "    topic_repos_dict={\n",
    "        'username':[],\n",
    "        'repo_name':[],\n",
    "        'stars':[],\n",
    "        'repo_url':[]\n",
    "        }\n",
    "    \n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info=get_repo_info(repo_tags[i],star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. scrape_topic(topic_url, path): This function is used to scrape and save topic-related repository data.\n",
    "\n",
    "- Checks if a file at the specified path already exists. If it does, it skips further processing.\n",
    "- Calls get_topic_repos to retrieve repository information from the topic page.\n",
    "- Writes the information to a CSV file at the specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url,path):\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping...\".format(path))\n",
    "        return\n",
    "    topic_df=get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The `scrape_topics_repos()` function automates the extraction of data related to GitHub topics and their top repositories. It first collects topic data and saves it in a DataFrame. Then, it iterates through these topics, scraping data for each one and storing it in CSV files within a 'data' directory. This function simplifies the process of collecting and organizing GitHub data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print(\"Scraping list of topics\")\n",
    "    topics_df=scrape_topics()\n",
    "    \n",
    "    os.makedirs('data',exist_ok=True)\n",
    "    for index,row in topics_df.iterrows():\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['titles']))\n",
    "        scrape_topic(row['url'],'data/{}.csv'.format(row['titles']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This project involves creating a program to collect information about topics and top repositories on GitHub. Here's what the code does in simple terms:\n",
    "\n",
    "1. It starts by gathering a list of GitHub topics, including their titles, descriptions, and URLs.\n",
    "\n",
    "2. It then goes on to scrape data about the top repositories for each of these topics. The data includes the username, repository name, number of stars, and repository URL.\n",
    "\n",
    "3. The scraped data is organized and saved in CSV files. Each CSV file contains information about a specific topic's top repositories.\n",
    "\n",
    "4. The program automates the entire process, making it easier to collect and store GitHub data for analysis and further use.\n",
    "\n",
    "In essence, it's a tool to fetch and save information about GitHub topics and their popular repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
